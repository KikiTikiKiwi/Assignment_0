{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mzw9gg0FlW22",
        "1iFmWEtQlcCJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KikiTikiKiwi/Assignment_0/blob/main/FinalProjectReport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ijb7S5IIbp9",
        "outputId": "9a832d7a-a02a-457f-fc61-d1fa1c109a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n"
      ],
      "metadata": {
        "id": "-z-R2KZQj5Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract:\n",
        "In recent decades, one of the most prominent topics of debates and discussions is using melee weapons, and firearms, in the United States of America. While using weapons in this country is legal, scientists are continuously trying to innovate and develop ways in which they can facilitate the paths for police to monitor and arrest the people who are trying to use these tools for criminal purposes.  \n",
        "\n",
        "In this project, we strived to train a model to detect people who unveil their weapons in public, which is considered a potentially hazardous situation even if the person has not aimed the gun at anyone.\n",
        "\n",
        "We aimed to use the most advanced techniques available to train the model, in order to get the optimal results. We used the Convolutional Neural Network (CNN) and YOLO (You Only Look Once) for our model. Although we were monitoring several different performance metrics, mAP_0.5, mAP_0.5:0.9, recall, precision, we focused on achieving high \"recall\" (low false negative), mainly because we needed to detect all tools, regardless of whether they were correctly identified as weapons. When we prioritized the “recall” as the most important performance metric, the model detected a few incorrect tools. Due to our sensitivity to our project, it was not an issue, as we needed to maintain a high accuracy as the recall increases.\n",
        "\n",
        "While some companies have worked on the concept of weapon detection such as Bosch Security and Safety Systems, ZeroEyes, Evolve, ... our project's uniqueness laid in splitting the melee weapons and firearms into several subclasses, which provided some advantages to the results like context-specific threat assessment."
      ],
      "metadata": {
        "id": "cP-Rmc98kSkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "ZMVVXuJqj4Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction:\n",
        "In today’s era of scientific advancement, not only are most people willing to use cameras to monitor their car, house, and businesses, but also the authorities and governments are eager to leverage the applied AI and Neural Network models to cameras to detect and become aware of potential threats through live feeds from streets, highways and public areas like malls and stadiums as soon as the threat starts to emerge.\n",
        "\n",
        "Deep learning architecture, as a sub-field of machine learning, relies on learning various features of the main data. The simple architecture of deep learning is Convolutional Neural Network, that includes convolutional layers, max pooling, activation function, dropout, fully connected layer, and classification layers.\n",
        "\n",
        "In recent years, the CNNs have stayed at the pinnacle of performance in terms of object detection, classification, and image segmentation. CNNs are widely used for object detection, primarily because they excel at extracting features from images, allowing for accurate object detection. The combination of CNNs with algorithms like Yolo (You Only Look Once), which is a single-stage way, provides higher speed and efficiency for real-time object detection. It makes the ideal situation for projects like weapon detection, in which the speed and accuracy of detection are vital.\n",
        "\n",
        "Our project presents a new model in the field of gun detection with detection of eight classes that are celurit, handgun, long-blade, machine gun, rifle, short-blade, shotgun, and submachine gun, based on deep learning, combination of CNN and YOLO."
      ],
      "metadata": {
        "id": "Ct_J9F9OkOoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description\n"
      ],
      "metadata": {
        "id": "5pyVsxijkUU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation:\n",
        " It is crucial to obtain a balanced set of data across all classes, which can impact the performance of a model significantly. 2 These images must align precisely with the detection goals outlined in the last section, or else it hinders the performance. By collecting images through Roboflow Universe, Image Scraping, and individually picking images from websites, we could compile a final dataset on its 14th iteration with at least 1,500 images per class spread across eight or more classes. Although re-annotating some of the images in downloaded ready-to-go datasets from Roboflow took much less time, manually annotating the downloaded images was unavoidable for some classes.\n",
        "\n",
        "<center>\n",
        "\n",
        "## Classes:\n",
        ">1. Celurite\n",
        "\n",
        "<left>\n",
        "  <img src=\"https://drive.google.com/uc?id=1siFWg41eq1C_M23EaclzEHFqT5USCWZU\" width=\"200\" title=\"My Image\">\n",
        "</left>\n",
        "\n",
        ">2. Handgun\n",
        "<div>\n",
        "  <left>\n",
        "      1wWS0XgyBLHW-hfOepHJXpp4BnFjXudoc\" width=\"200\">\n",
        "  </left>\n",
        "</div>\n",
        "\n",
        ">3. Long-Blade\n",
        "\n",
        "<left>\n",
        "    <img src=\"https://drive.google.com/uc?id=1H2wxh_v-Ow6J4r8-kRdPrX8NHv3WPdhR\" width=\"300\" title=\"My Image\">\n",
        "</left>\n",
        "\n",
        "\n",
        ">4. Machine-Gun\n",
        "\n",
        "<left>\n",
        "<img src=\"https://drive.google.com/uc?id=146_GvjTdFTbM_LITUjukcI9_QNmdGiEL\" width=\"300\">\n",
        "</left>\n",
        "\n",
        ">5. Rifle\n",
        "\n",
        "<left>\n",
        "<img src=\"https://drive.google.com/uc?id=1yQ_K49s7D7CyyHwDDvPVxC9WIm-4MqDC\" width=\"300\">\n",
        "</left>\n",
        "\n",
        ">6. Short-Blade\n",
        "\n",
        "<left>\n",
        "<img src=\"https://drive.google.com/uc?id=1pFeYgAJD6Y7tKlknG1OFv_E-F3U-ot9K\" width=\"300\">\n",
        "</left>\n",
        "\n",
        ">7. Shotgun\n",
        "\n",
        "<left>\n",
        "<img src=\"https://drive.google.com/uc?id=1QRldLVA7tvt7efY4v3tIPCFY1ZSGCgYm\" width=\"300\">\n",
        "</left>\n",
        "\n",
        ">8. Submachine Gun\n",
        "\n",
        "<left>\n",
        "<img src=\"https://drive.google.com/uc?id=1jSKX2HuFpAzhwXqsIUaIKtV2WlrDj9Kl\" width=\"300\">\n",
        "</left>\n",
        "\n",
        "</center>\n",
        "\n",
        "**Below** is an example of code used for image scraping websites for specific tags on photos. It is unknown what will be picked up so post processing and tossing redundant images is necessary before importing into RoboFlow.\n",
        "\n",
        "```\n",
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p \"/content/drive/My Drive/Colab Notebooks/Images\"\n",
        "\n",
        "\n",
        "URl =\"https://www.escoffier.edu/blog/culinary-arts/different-knives-and-the-best-uses-for-each/\"\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Images\")\n",
        "\n",
        "folder = 'knives'\n",
        "os.mkdir(os.path.join(os.getcwd(),folder))\n",
        "\n",
        "r = requests.get(URl)\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "os.chdir(os.path.join(os.getcwd(),folder))\n",
        "print(soup.title.text)\n",
        "\n",
        "images = soup.find_all('img')\n",
        "for image in images:\n",
        "    name = image.get('alt', 'default_name')\n",
        "    link = image.get('data-lazy-src', image.get('src'))\n",
        "    if not link:\n",
        "        continue\n",
        "    print(link)\n",
        "\n",
        "    # if not link.startswith('http'):\n",
        "    #   link = 'https:' + link if link.startswith('//') else 'https://' + link\n",
        "\n",
        "    try:\n",
        "      with open(name.replace(' ','-').replace('/','') + '.jpg', 'wb') as f:\n",
        "          im = requests.get(link)\n",
        "          f.write(im.content)\n",
        "    except requests.exceptions.InvalidSchema:\n",
        "        print(f\"Ignoring invalid schema for: {link[:20]}...\") # Print message\n",
        "        pass  # Ignore InvalidSchema errors and continue the loop\n",
        "```\n",
        "\n",
        "##Image Resolution:\n",
        " An input size of 640X640 was used in compiling the dataset, as this was what Roboflow offered as the recommended value. A high enough resolution must be used to maximize any model's training and feature selection for small objects. However, downscaling high-resolution raw images to lower resolutions can degrade the model's ability to detect delicate details. Like the image below, its very grainy.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1E3BAVU3TNsE5vMZQuKQ0aREx-VgVhLps\" width=\"300\" title=\"My Image\">\n",
        "<p><i>Figure 1: A sample image of [insert description here].</i></p>\n",
        "</center>\n",
        "\n",
        "While on the topic of data, the above image is an example of badly picked photos. Unless we wanted the model to recognize weapons in certain situations, then we could include the hand grip or certain items in the annotated box. However too much external information of the image is included with the object, which can hinder the model.\n",
        "\n",
        "## Augmentation:\n",
        "Augmentation: Next, in collecting and processing images, the augmentation feature tool in Roboflow was utilized to grow the size of the dataset. Blur, rotation, shear, and exposure were consistent throughout creating multiple datasets. However, other augmentation features were played around with and found to impact the model's performance negatively.\n",
        "\n",
        "> 1. Mosaic: Works to combine images in a collage-like photo. However, **it was found** in the hyperparameter settings that this was an online feature that, if applied twice, dramatically hindered the model's performance. In the example below, it was a combination of mosaic and the tiling augmentation that created this unique combination that we selectively cropped to show.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1QitSVPfxoM8Ss3BxTti2gxGOBMv0W22m\" width=\"300\">\n",
        "<p><i>Figure 2: A sample image of [insert description here].</i></p>\n",
        "</center>\n",
        "\n",
        "\n",
        "> 2. Cutout: Removes little chunks of the image, to simulate obstruction of images. Having this set too high may be bad for the training of the model as too much information might be lost. For this reason we opted from moving forward with it.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1BceX7mLQmDX2zy8pnChIZH5kMDnNz00S\" width=\"300\">\n",
        "<p><i>Figure 3: A sample image of [insert description here].</i></p>\n",
        "</center>\n",
        "   \n",
        "    \n",
        "## Data Cleaning:\n",
        " Cleaning of the images means combing through the dataset and removing images deemed to be low quality. Low-quality images can hinder the model’s ability to learn relevant features, especially when dealing with small objects like knives and the intricate details of guns. A huge part of training a YOLO model is annotating using bounded boxes that are tightly bounded around the object, minimizing the captured area around the object itself. Here is what must be adhered to....\n",
        "\n",
        ">1. Bounding boxes do not overlap unnecessarily with irrelevant parts of the image.\n",
        "\n",
        ">2. Objects are not truncated or misaligned within the boxes.\n",
        ">3. Labels are consistent and correctly assigned to their respective objects."
      ],
      "metadata": {
        "id": "QnOiW-wYmzWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "_AKRXZshkdEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1bg98e8WElTiiMa72lrgDOfXn1U43FHyg\" width=\"400\">\n",
        "<p><i>Figure 4: YOLOv5 Architecture (Tang 2015).</i></p>\n",
        "</center>\n",
        "\n",
        "Variations of YOLOv5 models were used but the small variant was thought to be the best starting point as a baseline - Which is computationally efficient and well-suited for tasks requiring real-time performance. The layers and parameters are as follows with no changes or freezes to them:  \n",
        "\n",
        ">1. Backbone: CSPDarknet53, which extracts features using convolutional layers. The backbone then generates different sized feature maps which fuse with feature maps in the neck.\n",
        "\n",
        ">2. Neck: PANet (Path Aggregation Network) for multi-scale feature fusion, enabling better object detection across varied object sizes (small, medium,large).\n",
        "\n",
        ">3. Head: Final detection layers use anchor boxes to predict bounding boxes, class probabilities, and confidence scores. Through the use of three prediction heads which detect different sized objects....\n",
        "\n",
        "##  Default YOLOv5 Variants\n",
        " In addition to the baseline, we experimented with larger variants (YOLOv5-medium and YOLOv5-large). Then in a last-ditch attempt, we moved to the nano variant.\n",
        "\n",
        "## Custom YOLOv5 Variants\n",
        "NEED to source paper Here we used an added layer between the backbone and neck architecture, called Involution, in the hopes of performance enhancement on smaller object detection. Specifically, if you have added an involution block, the paper by Shiyi Tang's group demonstrates that it significantly reduces channel information loss while increasing spatial adaptability.  "
      ],
      "metadata": {
        "id": "5kOkdfQ3lRwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "0jGX0Pm1lNas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hardware Setup and Importance\n",
        "The importance of powerful machines for object detction can not be oversatted. Without these tools, training time would just take too long on local machines. You either must purchase run time on external servers or buy your own special setup to conduct these trains. When we tried to monitor train times on local machines, we were getting 12+ hours or more compared to 1-2 hours on this set up mentioned below.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?id=1wpctHQEkN5sFsBAQ96KMrctq7vPC2k2V\" width=\"500\">\n",
        "<p><i>Figure 5: <font size=\"2\">GPU utilization and memory status of two NVIDIA RTX A4000 GPUs, showing real-time usage and processes for efficient resource management in machine learning tasks.</i></p>\n",
        "</center>\n",
        "\n",
        "> 1. **High Memory Capacity:**\n",
        "High Memory Capacity: GPUs like the NVIDIA RTX A4000 come with large memory (16GB in total), which is beneficial when training large models on large datasets. **Which means** we are able to conduct training processes with huge batches of images.\n",
        "\n",
        ">2. **Optimizing Training Time:**\n",
        " You can enhance training efficiency by using two GPUs. Each GPU can process different parts of the data or manage different model components, significantly speeding up the training process compared to using just one GPU. An added benefit of the increased speed is the multiple trains one can do in a given time frame.\n",
        "\n",
        "\n",
        "\n",
        "## Training Methodology:\n",
        "The thinking on training is the idea of “good” data. For a train to be successful, it is not initially on the tunning of hyperparameters but instead on the ability of the model to generalize features from reliable source material. For instance, evaluating the general performance, such as (mAP) from the precision-recall curve (insert image of exp2 from old runs) we observe high-performing classes like AK, machine guns, and celurit, but we also observe low performing classes such as rifle and submachine guns. A telling sign of the model's performance is the very steep decline in precision as recall increases, meaning we have poor consistency across the dataset. This leads to 3 steps one can take or a combination of these three. Extra data augmentations address class imbalances and increase annotation quality.\n",
        "\n",
        ">1. Data Augmentation: For low-performing classes, it’s a sign to add diversity of samples (images) with blur, exposure, and rotations…. which acts to increase class instances but also adds variation to help generalize the class's features.\n",
        "\n",
        ">2. Class Imbalance: More than likely, a class imbalance hinders the model generalization. If we compare the count of instances for rifle and submachine guns, we notice they are lower than other classes. To address this problem more images are to be added.\n",
        "\n",
        ">3. Annotation Quality: The best annotations are those that cover all parts and edges of the object with the lowest portion of unwanted objects in the bounding boxes. Verifying that the annotations are of a decent quality and accurate will also do much to improve any model. Errors in bounding boxes not being tight enough and mislabeling can harm the model.\n",
        "\n",
        "With these three improvements in mind, a second dataset was created, combination of AR and AK variants into one large class of rifle. More augmentations were used to increase the count of instances of each image and more images were added to help balance the classes. Painstakingly, each image had to be looked at, adjusting the annotations of the boxes which encompassed too much of the surrounding image itself.\n",
        "\n",
        "## Future Iterations of Training:\n",
        "After each iteration of training, the **method** is is too modify the dataset each time, following the above ideas. As a side note: The dataset is on its 14th version, which aligns with the total runs done using the default YOLOv5 variants (more or less).  \n",
        "\n",
        "> ### LOSS Function\n",
        "\n",
        "> ### Optimization Used\n",
        "\n",
        "\n",
        "## Changing the Hypermeters:\n",
        "The hyperparameters were only changed once the dataset was thought to be okay enough to start playing around with those features. Furthermore, they were only changed for the models using the involution block in the custom YOLOv5s structure variant. Learning rate, iou_t, and anchor_t were the hyperparameters changed that we believed would affect the model. We compare the baseline custom YOLOv5s involution block variant using default hyperparameters to any runs using custom hyperparameter files to observe if any beneficial changes occurred from these adjustments. Models are trained using the baseline's weights to speed up training because we are only concerned with improvements.  \n",
        "\n"
      ],
      "metadata": {
        "id": "alvAa7--lQzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from roboflow import Roboflow\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from pytube import YouTube\n",
        "\n",
        "%cd /home/kieran/Documents/NN_SemesterProject_Final\n",
        "loc = \"home/kieran/Documents/NN_SemesterProject_Final\"\n",
        "\n",
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "id": "XklJ5iRXFFSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here were are downloading the dataset from roboflow onto our local drive\n",
        "\n",
        "%cd /home/kieran/Documents/NN_SemesterProject_Final/yolov5\n",
        "\n",
        "# RoboFlow Key to dowload dataset into current directory\n",
        "rf = Roboflow(api_key=\"r2ak1XEFMW9I2efwpenJ\")\n",
        "project = rf.workspace(\"rob-ougsp\").project(\"weapons-classification-crdyf\")\n",
        "version = project.version(15)\n",
        "dataset = version.download(\"yolov5\")"
      ],
      "metadata": {
        "id": "UBUpUlp9FRRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comand line to train the model\n",
        "# Using multiple GPUs to help speed up the process\n",
        "\n",
        "!python -m torch.distributed.run --nproc_per_node 2 train.py --batch 128 --weights runs/train/exp9/weights/best.pt --epoch 50 --imgsz 640 --data {dataset.location}/data.yaml --cfg models/Custom_yolov5s.yaml --device 0,1 --cache ram"
      ],
      "metadata": {
        "id": "XVX1SdblFUcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed in a collection of photos or a video to evaluate the model's ability to predict\n",
        "!python detect.py --conf 0.25 --imgsz 640 --source knife_2.mp4 --weights /home/kieran/Documents/NN_SemesterProject_Final/yolov5/runs/train/exp9/weights/best.pt"
      ],
      "metadata": {
        "id": "d1FVFcnwFo73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Analysis"
      ],
      "metadata": {
        "id": "xkomvSzglTnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultimatly, the data can be through of as having been broken up into **three stages.**\n",
        ">1. The first being to test out what machines we should run the training on and what the results would look like. Secondly we also wanted to gain a feel for the dataset.\n",
        "\n",
        ">2. The second stage was with improved data, where variations of the yolov5 structures were tested out.  \n",
        "\n",
        ">3. The third stage was using the involutional block to\n",
        "customize the yolov5 small variant and to test metout changing hyperparaers.\n",
        "\n"
      ],
      "metadata": {
        "id": "9CIke_KggsZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Run of First Stage\n",
        "Here, we will analyze the baseline run and the steps taken from this point to the next.\n",
        "\n",
        "### **1. Training Run Information**\n",
        "- **Run ID/Name:** *(Run #1, \"Baseline Model\")*\n",
        "- **Model Version:** *(YOLOv5s)*\n",
        "- **Training Dataset:** *(Weapons-Classification--2)*\n",
        "- **Purpose of the Run:** *(e.g., testing new the dataset and setting a baseline)*\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Training Configuration**\n",
        "- **Model Parameters:**\n",
        "  - Backbone Architecture: CSPDarknet53\n",
        "  - Number of Classes: 11\n",
        "- **Hyperparameters:**\n",
        "  - Learning Rate: 0.01\n",
        "  - Batch Size: 16\n",
        "  - Epochs: 100\n",
        "  - Image Size: 256\n",
        "  - Optimizer Used *(SGD)*\n",
        "- **Augmentation Settings:** *(rotation, blur, mosaic X2, scale )*\n",
        "- **Pre-trained Weights:** *(Yes; COCO weights(yolov5s.pt))*\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Training Logs Overview**\n",
        "- **Confusion Matrix**\n",
        "<div>\n",
        "  <img src=\"https://drive.google.com/uc?id=1pQYZY2Ys5Px9VH8c8AbduL4yqKP7jLWl\" width=\"300\" title=\"My Image\">\n",
        "  <p><i>Figure 1: A description of the image goes here.</i></p>\n",
        "</div>\n",
        "\n",
        "- **Loss Curves:**\n",
        "<div>\n",
        "  <img src=\"https://drive.google.com/uc?id=1iJTEFkVk9crS-i8C1680kQnmRiB_JQ8A\" width=\"300\">\n",
        "  <p><i>Figure 1: A description of the image goes here.</i></p>\n",
        "</div>\n",
        "\n",
        "- **Validation Curves:**\n",
        "<div>\n",
        "  <img src=\"https://drive.google.com/uc?id=1XL106k9amiRAdU0UIwr591NPm-pSJwAi\" width=\"300\" title=\"My Image\">\n",
        "  <p><i>Figure 1: A description of the image goes here.</i></p>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Qualitative Evaluation**\n",
        "- **Detection Examples:**\n",
        "  - Include sample images showing bounding box predictions.\n",
        "  - Discuss strengths and weaknesses in predictions.\n",
        "\n",
        "- **Failure Cases:**\n",
        "  - Highlight images where the model struggled (e.g., false positives, missed detections).\n",
        "\n",
        "- **Class-wise Analysis:**\n",
        "  - Which classes performed well? Which classes require improvement?\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Comparative Analysis**\n",
        "- **Comparison to Previous Runs:**\n",
        "  - How does this run compare to earlier runs in terms of performance, loss, or training speed?\n",
        "- **Effect of Changes:**\n",
        "  - Highlight the impact of any specific changes (e.g., new dataset, hyperparameter tuning, data augmentation).\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Next Steps and Recommendations**\n",
        "- **Improvement Areas:**\n",
        "  - Suggestions for hyperparameter tuning, dataset enhancements, or architectural changes.\n",
        "- **Future Experiments:**\n",
        "  - Planned adjustments for the next training run.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g8jns_PQlXh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Run of Second Stage\n",
        "\n",
        "### **1. Training Run Information**\n",
        "- **Run ID/Name:** *(Run #1, \"Baseline Model\")*\n",
        "- **Model Version:** *(YOLOv5s)*\n",
        "- **Training Dataset:** *(Weapons-Classification--2)*\n",
        "- **Purpose of the Run:** *(e.g., testing new the dataset and setting a baseline)*\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Training Configuration**\n",
        "- **Model Parameters:**\n",
        "  - Backbone Architecture: CSPDarknet53\n",
        "  - Number of Classes: 11\n",
        "- **Hyperparameters:**\n",
        "  - Learning Rate: 0.01\n",
        "  - Batch Size: 16\n",
        "  - Epochs: 100\n",
        "  - Image Size: 256\n",
        "  - Optimizer Used *(SGD)*\n",
        "- **Augmentation Settings:** *(rotation, blur, mosaic X2, scale )*\n",
        "- **Pre-trained Weights:** *(Yes; COCO weights(yolov5s.pt))*\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Training Logs Overview**\n",
        "- **Confusion Matrix**\n",
        "<div>\n",
        "  <img src=\"https://drive.google.com/uc?id=1pQYZY2Ys5Px9VH8c8AbduL4yqKP7jLWl\" width=\"300\" title=\"My Image\">\n",
        "  <p><i>Figure 1: A description of the image goes here.</i></p>\n",
        "</div>\n",
        "\n",
        "- **Loss Curves:**\n",
        "<div>\n",
        "  <img src=\"https://drive.google.com/uc?id=1iJTEFkVk9crS-i8C1680kQnmRiB_JQ8A\" width=\"300\">\n",
        "  <p><i>Figure 1: A description of the image goes here.</i></p>\n",
        "</div>\n",
        "\n",
        "- **Validation Curves:**\n",
        "<div>\n",
        "  <img src=\"https://drive.google.com/uc?id=1XL106k9amiRAdU0UIwr591NPm-pSJwAi\" width=\"300\" title=\"My Image\">\n",
        "  <p><i>Figure 1: A description of the image goes here.</i></p>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Qualitative Evaluation**\n",
        "- **Detection Examples:**\n",
        "  - Include sample images showing bounding box predictions.\n",
        "  - Discuss strengths and weaknesses in predictions.\n",
        "\n",
        "- **Failure Cases:**\n",
        "  - Highlight images where the model struggled (e.g., false positives, missed detections).\n",
        "\n",
        "- **Class-wise Analysis:**\n",
        "  - Which classes performed well? Which classes require improvement?\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Comparative Analysis**\n",
        "- **Comparison to Previous Runs:**\n",
        "  - How does this run compare to earlier runs in terms of performance, loss, or training speed?\n",
        "- **Effect of Changes:**\n",
        "  - Highlight the impact of any specific changes (e.g., new dataset, hyperparameter tuning, data augmentation).\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Next Steps and Recommendations**\n",
        "- **Improvement Areas:**\n",
        "  - Suggestions for hyperparameter tuning, dataset enhancements, or architectural changes.\n",
        "- **Future Experiments:**\n",
        "  - Planned adjustments for the next training run.\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kXsWRiVvyzjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion"
      ],
      "metadata": {
        "id": "mzw9gg0FlW22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ms-9p2a3lbbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "1iFmWEtQlcCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIVV2kKslf3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "Akash, S. A. A., Moorthy, R. S. S., Esha, K., & Nathiya, N. (2022). Human Violence Detection Using Deep Learning Techniques. Journal of Physics: Conference Series, 2318(1), 012003. https://doi.org/10.1088/1742-6596/2318/1/012003\n",
        "\n",
        "How to increase recall · Issue #2449 · ultralytics/yolov5. (n.d.). GitHub. Retrieved November 20, 2024, from https://github.com/ultralytics/yolov5/issues/2449\n",
        "\n",
        "Jocher, G., Chaurasia, A., Stoken, A., Borovec, J., NanoCode012, Kwon, Y., Michael, K., TaoXie, Fang, J., imyhxy, Lorna, Yifu), 曾逸夫(Zeng, Wong, C., V, A., Montes, D., Wang, Z., Fati, C., Nadar, J., Laughing, … Jain, M. (2022). ultralytics/yolov5: V7.0 - YOLOv5 SOTA Realtime Instance Segmentation (Version v7.0) [Computer software]. Zenodo. https://doi.org/10.5281/zenodo.7347926\n",
        "\n",
        "Mahasin, M., & Dewi, I. A. (2022). Comparison of CSPDarkNet53, CSPResNeXt-50, and EfficientNet-B0 Backbones on YOLO V4 as Object Detector. International Journal of Engineering, Science and Information Technology, 2(3), 64–72. https://doi.org/10.52088/ijesty.v2i3.291\n",
        "Tang, S., Zhang, S., & Fang, Y. (2023). HIC-YOLOv5: Improved YOLOv5 For Small Object Detection (No. arXiv:2309.16393). arXiv. https://doi.org/10.48550/arXiv.2309.16393\n",
        "\n",
        "Wang, C.-Y., Bochkovskiy, A., & Liao, H.-Y. M. (2021). Scaled-YOLOv4: Scaling Cross Stage Partial Network (No. arXiv:2011.08036). arXiv. http://arxiv.org/abs/2011.08036\n",
        "\n",
        "YOLOv5 Object Detection Model: What is, How to Use. (n.d.). Retrieved October 27, 2024, from https://roboflow.com/model/yolov5\n"
      ],
      "metadata": {
        "id": "TNFUXV2UlgKs"
      }
    }
  ]
}